# Cache 与 Event Handling 走读

本文梳理 KAI Scheduler 的数据层：即如何通过 `Cache` 维护集群状态，以及如何通过 `StatusUpdater` 和 `Events` 与外界（K8s API Server）交互。

## 1. 架构总览

调度器是无状态的（Stateless），但在每个调度周期（Scheduling Session）中，它需要一份集群的“快照（Snapshot）”。

`SchedulerCache` 是核心组件，它负责：

1. **Informer 监听**: 通过 K8s SharedInformer 监听 Pod, Node, PodGroup, Queue 等资源的变更。
2. **Snapshot 生成**: 在每个 Session 开始时，提供一份只读的集群快照（`api.ClusterInfo`）。
3. **指令执行**: 封装对 K8s 的写操作，如 Bind (调度), Evict (抢占), UpdateStatus (状态更新)。

---

## 2. 核心组件：SchedulerCache

代码位置：`pkg/scheduler/cache/cache.go`

### 2.1 初始化

在 `NewScheduler` 中初始化，包含：

- **KubeClient / KAIClient**: 与 API Server 交互。
- **InformerFactories**: 启动各类资源的监听。
- **Evictor**: 负责 Pod 驱逐。
- **StatusUpdater**: 异步更新 Pod/PodGroup 状态。
- **ClusterInfo**: 维护内存中的集群状态模型。

### 2.2 运行循环 (Run)

`Run` 方法启动所有 Informer 和 Worker：

```go
func (sc *SchedulerCache) Run(stopCh <-chan struct{}) {
    sc.informerFactory.Start(stopCh)
    // ... 启动其他 informer ...
    sc.StatusUpdater.Run(stopCh)
}
```

---

## 3. 快照生成流程 (Snapshot)

代码位置：`pkg/scheduler/cache/cluster_info/cluster_info.go`

`Snapshot()` 方法是调度周期的起点。它从 Informer 的 Cache 中拉取数据，并组装成调度器内部的 `Model`。

**步骤序列**：

1. **List Pods**: 获取所有 Pod。
2. **Snapshot Nodes**: 获取所有 Node，并处理 Affinity Info。
3. **Snapshot BindRequests**: 获取待处理的 Bind 请求。
4. **Add Tasks To Nodes**: 将 Pod 映射到 Node 上（计算资源占用）。
5. **Snapshot Queues**: 获取 Queue 并构建层级关系。
6. **Snapshot PodGroups**:
   - 核心步骤！将 Pod 归类到 PodGroup。
   - 验证 PodGroup 是否由当前调度器负责（`isPodGroupUpForScheduler`）。
   - 计算 Priority 和 Preemptibility。

**关键点**：Snapshot 一旦生成，在当前调度 Session 内是不可变的（Immutable）。这保证了调度逻辑的一致性。

---

## 4. 事件处理与状态更新 (Status Updater)

代码位置：`pkg/scheduler/cache/status_updater/default_status_updater.go`

为了避免阻塞调度主循环，状态更新通过 `StatusUpdater` 异步并发处理。

### 4.1 异步队列模型

- **Channels**: `updateQueueIn` -> `workers` -> `APIServer`。
- **Concurrency**: 支持多 Worker 并发写入。
- **Patch/Update**: 智能选择 Patch 操作以减小开销。

### 4.2 关键操作

- **Bind**: 绑定 Pod 到节点。如果失败，记录 Event 并更新 Pod Condition。
- **Evict**: 调用 K8s Eviction API 驱逐 Pod。记录 `Evicted` 事件（包含 `num-evicted-pods` 等元数据）。
- **RecordJobStatusEvent**:
  - 更新 PodGroup 的 Annotations (StaleTime, StartTime)。
  - 更新 `SchedulingConditions`（如 `Unschedulable` 原因）。
  - **Deduplication**: 如果状态没变，为了减少 API 调用，会跳过更新。

---

## 5. UsageDB (Prometheus 集成)

代码位置：`pkg/scheduler/cache/usagedb/usagedb.go`

这是一个可选组件，用于从 Prometheus 等外部数据源获取集群的**真实资源使用率**（Real-time Metrics），而非 Request/Limit。
这对于实现“基于真实负载的超卖”或“动态资源调整”至关重要。

流程：

1. **Fetch**: 定期（默认 1 分钟）从 Prometheus 拉取数据。
2. **Cache**: 缓存在内存中 (`lastUsageData`)。
3. **Usage**: 在 `Snapshot` 时注入到 `ClusterInfo` 中供插件使用。

---

## 6. 总结

`SchedulerCache` 是调度器的“心脏”：

- **输入**: 通过 Informer 把 K8s 对象转化为内部 Model。
- **输出**: 通过 StatusUpdater/Binder 把调度决策写回 K8s。
- **一致性**: 通过 Snapshot 机制隔离了外部变化，让调度器内核可以专心做决策。
