## KAI Scheduler 调度主线走读：从 main 到 Allocate

本文记录一次从 `cmd/scheduler/main.go` 出发，到 `scheduler.go`、`actions/allocate.go`、`actions/common/allocate.go` 的源码走读主线，方便之后回顾，不被细节问题打断思路。

---

## 1. 进程入口：`cmd/scheduler/main.go`

调度器二进制的入口非常简单：

```go
func main() {
	if err := app.RunApp(); err != nil {
		fmt.Fprintf(os.Stderr, "%v\n", err)
		os.Exit(1)
	}
}
```

- **职责**：仅负责调用 `app.RunApp()` 并处理返回错误。
- **含义**：所有实质逻辑（解析参数、初始化 client、启动调度循环等）都在 `cmd/scheduler/app` 包内完成。

---

## 2. `app.RunApp` / `app.Run`：解析参数 + 初始化 Scheduler

文件：`cmd/scheduler/app/server.go`

### 2.1 `RunApp`：命令行参数与基础设施

关键逻辑：

```go
func RunApp() error {
	so := options.NewServerOption()
	so.AddFlags(pflag.CommandLine)
	if err := so.ValidateOptions(); err != nil { ... }

	mux := http.NewServeMux()
	go func() {
		_ = http.ListenAndServe(fmt.Sprintf(":%d", so.PluginServerPort), mux)
	}()

	setupProfiling(so)
	if err := setupLogging(so); err != nil { ... }
	setConfig(so)

	config := clientconfig.GetConfigOrDie()
	config.QPS = float32(so.QPS)
	config.Burst = so.Burst

	return Run(so, config, mux)
}
```

- **ServerOption**：通过 `options.ServerOption` + `pflag` 解析命令行。这里会处理：
  - `--scheduler-name`
  - `--namespace`
  - `--scheduler-conf`
  - QPS / Burst 等。
- **HTTP mux**：起一个内部 HTTP server，用于插件交互（`PluginServerPort`）。
- **日志 / 性能分析**：`setupProfiling`、`setupLogging` 完成 profiling 和日志的初始化。
- **KubeConfig**：用 `clientconfig.GetConfigOrDie()` 获取集群配置，并设置 QPS / Burst，再交给 `Run`。

### 2.2 `Run`：注册 actions / plugins，构造 Scheduler 并进入 leader election

核心片段：

```go
func Run(opt *options.ServerOption, config *restclient.Config, mux *http.ServeMux) error {
	if opt.PrintVersion {
		version.PrintVersion()
	}
	metrics.InitMetrics(opt.MetricsNamespace)

	actions.InitDefaultActions()
	plugins.InitDefaultPlugins()

	schedConfig, err := conf_util.ResolveConfigurationFromFile(opt.SchedulerConf)
	...

	scheduler, err := scheduler.NewScheduler(
		config,
		schedConfig,
		BuildSchedulerParams(opt),
		mux,
	)
	...

	go func() {
		http.Handle("/metrics", promhttp.Handler())
		glog.Fatalf("Prometheus Http Server failed %s", http.ListenAndServe(opt.ListenAddress, nil))
	}()

	run := func(ctx context.Context) {
		scheduler.Run(ctx.Done())
		<-ctx.Done()
	}

	// 后面使用 Kubernetes leader election，拿到 lease 后执行 run()
}
```

- **初始化动作与插件**：
  - `actions.InitDefaultActions()`：注册诸如 `Allocate`、`Preempt`、`Reclaim` 等调度动作。
  - `plugins.InitDefaultPlugins()`：注册策略插件。
- **加载 scheduler 配置**：
  - 通过 `conf_util.ResolveConfigurationFromFile(opt.SchedulerConf)` 解析调度配置文件（包括 actions 序列、相关参数等）。
- **构造 `Scheduler` 实例**：
  - 使用 `scheduler.NewScheduler(config, schedConfig, BuildSchedulerParams(opt), mux)`。
- **Metrics HTTP Server**：
  - 在 `opt.ListenAddress` 上暴露 `/metrics`，给 Prometheus 抓取。
- **Leader Election**：
  - 下面一大段配置 `resourcelock.New` 和 `leaderelection.RunOrDie`。
  - 成为 leader 后会调用 `run(ctx)`，真正进入调度循环。

---

## 3. `pkg/scheduler/scheduler.go`：Scheduler 结构与调度循环

文件：`pkg/scheduler/scheduler.go`

### 3.1 `Scheduler` 结构体与 `NewScheduler`

#### 3.1.1 `Scheduler` 结构体

```go
type Scheduler struct {
	cache           schedcache.Cache
	config          *conf.SchedulerConfiguration
	schedulerParams *conf.SchedulerParams
	schedulePeriod  time.Duration
	mux             *http.ServeMux
}
```

各字段含义：

| 字段 | 类型 | 作用 |
|------|------|------|
| `cache` | `schedcache.Cache` | 调度器的“世界观”：维护 Pod、PodGroup、Queue、Node 等集群状态的本地缓存，由 informer 驱动更新。后续 `runOnce` 会从 cache 构建 `Session` 的视图。 |
| `config` | `*conf.SchedulerConfiguration` | 来自 **scheduler 配置文件**（`--scheduler-conf` 指向的 YAML）。定义 actions 执行顺序、tiers/plugins、UsageDB 等。 |
| `schedulerParams` | `*conf.SchedulerParams` | 来自 **命令行参数**（`app.BuildSchedulerParams` 构建）。包含调度周期、节点池、是否限制节点、是否开启 CSI 调度等运行时参数。 |
| `schedulePeriod` | `time.Duration` | 调度周期，默认 1 秒。`Run` 中会每隔该周期调用一次 `runOnce`。 |
| `mux` | `*http.ServeMux` | 与 `cmd/scheduler/app` 中启动的 HTTP server 共用，用于插件服务（如 plugin server 的端口）。 |

**config 与 schedulerParams 的区别**：`config` 偏向“策略配置”（动作链、插件、UsageDB），`schedulerParams` 偏向“运行参数”（周期、节点池、功能开关）。

---

#### 3.1.2 `SchedulerConfiguration` 与 `SchedulerParams` 结构

`config` 对应的 `SchedulerConfiguration`（`pkg/scheduler/conf/scheduler_conf.go`）：

```go
type SchedulerConfiguration struct {
	Actions            string                    // 动作列表，如 "allocate, preempt, reclaim"
	Tiers              []Tier                    // 插件分层配置
	QueueDepthPerAction map[string]int            // 每个 action 每队列最大尝试 job 数
	UsageDBConfig      *usagedbapi.UsageDBConfig  // UsageDB 客户端配置（可选）
}
```

`schedulerParams` 对应的 `SchedulerParams`：

```go
type SchedulerParams struct {
	SchedulerName                     string
	RestrictSchedulingNodes           bool
	PartitionParams                   *SchedulingNodePoolParams  // NodePool 标签过滤
	MaxNumberConsolidationPreemptees  int
	ScheduleCSIStorage                bool
	UseSchedulingSignatures           bool
	FullHierarchyFairness             bool
	AllowConsolidatingReclaim         bool
	NumOfStatusRecordingWorkers       int
	GlobalDefaultStalenessGracePeriod  time.Duration
	SchedulePeriod                    time.Duration
	DetailedFitErrors                 bool
	UpdatePodEvictionCondition        bool
}
```

---

#### 3.1.3 `NewScheduler` 构造函数流程

```go
func NewScheduler(
	config *rest.Config,
	schedulerConf *conf.SchedulerConfiguration,
	schedulerParams *conf.SchedulerParams,
	mux *http.ServeMux,
) (*Scheduler, error) {
	// 1. 创建三套 K8s 风格 client
	kubeClient, kubeAiSchedulerClient, kueueClient := newClients(config)

	// 2. Discovery client（用于检测 API 版本、资源是否存在等）
	discoveryClient, err := discovery.NewDiscoveryClientForConfig(config)
	...

	// 3. UsageDB client（若配置了 UsageDB，用于资源使用统计）
	usageDBClient, err := getUsageDBClient(schedulerConf.UsageDBConfig)
	...
	var usageDBParams *api.UsageParams
	if schedulerConf.UsageDBConfig != nil {
		usageDBParams = schedulerConf.UsageDBConfig.GetUsageParams()
	}

	// 4. 组装 cache 参数并创建 cache
	schedulerCacheParams := &schedcache.SchedulerCacheParams{
		KubeClient:                  kubeClient,
		KAISchedulerClient:          kubeAiSchedulerClient,
		KueueClient:                 kueueClient,
		UsageDBParams:               usageDBParams,
		UsageDBClient:               usageDBClient,
		SchedulerName:               schedulerParams.SchedulerName,
		NodePoolParams:              schedulerParams.PartitionParams,
		RestrictNodeScheduling:      schedulerParams.RestrictSchedulingNodes,
		DetailedFitErrors:           schedulerParams.DetailedFitErrors,
		ScheduleCSIStorage:          schedulerParams.ScheduleCSIStorage,
		FullHierarchyFairness:       schedulerParams.FullHierarchyFairness,
		NumOfStatusRecordingWorkers: schedulerParams.NumOfStatusRecordingWorkers,
		UpdatePodEvictionCondition:  schedulerParams.UpdatePodEvictionCondition,
		DiscoveryClient:             discoveryClient,
	}

	scheduler := &Scheduler{
		config:          schedulerConf,
		schedulerParams: schedulerParams,
		cache:           schedcache.New(schedulerCacheParams),
		schedulePeriod:  schedulerParams.SchedulePeriod,
		mux:             mux,
	}

	return scheduler, nil
}
```

**步骤说明**：

1. **`newClients(config)`**：创建三套 client：
   - `kubeClient`：标准 Kubernetes client（Pod、Node、Event 等），对 built-in 资源强制使用 protobuf 序列化以提升性能。
   - `kubeAiSchedulerClient`：KAI 自定义 CRD 的 client（PodGroup、Queue、SchedulerConfig 等）。
   - `kueueClient`：Kueue 的 client（若集群中使用了 Kueue 做队列管理）。

2. **`discoveryClient`**：用于动态发现集群 API 版本和资源，cache 在初始化 informer 时可能用到。

3. **`usageDBClient` / `usageDBParams`**：若 `schedulerConf.UsageDBConfig` 非空，则创建 UsageDB 客户端，供 cache 做资源使用统计（如按项目/部门统计 GPU 使用量）。

4. **`schedcache.New(schedulerCacheParams)`**：创建 `SchedulerCache`，内部会：
   - 启动 Pod、Node、PodGroup、Queue 等 informer；
   - 维护 `clusterInfo`（Node、PodGroup、Queue 的聚合视图）；
   - 创建 `Evictor`、`StatusUpdater` 等组件。

5. **组装 `Scheduler`**：将 `config`、`schedulerParams`、`cache`、`schedulePeriod`、`mux` 填入结构体并返回。

---

#### 3.1.4 Cache 与后续调度的关系

`cache` 是调度循环的数据基础：

- `Scheduler.Run` 会先调用 `s.cache.Run(stopCh)` 启动 informer，再 `WaitForCacheSync` 等待同步完成。
- `runOnce` 中通过 `framework.OpenSession(s.cache, s.config, s.schedulerParams, ...)` 从 cache 构建本轮的 `Session`，Session 中的 `Queues`、`PodGroupInfos`、`Nodes` 等均来自 cache 的当前快照。

### 3.2 调度循环：`Run` 与 `runOnce`

本节对应 `docs/developer/scheduler-concepts-zh.md` 中的「调度周期」概念。调度器以**周期**方式运行，每个周期获取集群状态快照，并通过一系列 Actions 做出调度决策。

#### 3.2.1 与文档中的调度周期流程对应

`scheduler-concepts-zh.md` 给出的流程为：

```
周期开始 → Cache 同步 → 生成 Snapshot → 打开 Session → 执行 Actions → 关闭 Session → 周期结束
```

| 文档步骤 | 代码位置 |
|----------|----------|
| Cache 同步 | `Run` 中的 `cache.Run` + `WaitForCacheSync`（在首次 `runOnce` 前完成） |
| 生成 Snapshot | `framework.OpenSession` 内部的 `openSession` → `cache.Snapshot()` |
| 打开 Session | `framework.OpenSession` |
| 执行 Actions | `for _, action := range actions { action.Execute(ssn) }` |
| 关闭 Session | `framework.CloseSession(ssn)` |

---

#### 3.2.2 `Run`：启动 Cache 与周期循环

```go
func (s *Scheduler) Run(stopCh <-chan struct{}) {
	s.cache.Run(stopCh)
	s.cache.WaitForCacheSync(stopCh)

	go func() {
		wait.Until(s.runOnce, s.schedulePeriod, stopCh)
	}()
}
```

- **`cache.Run(stopCh)`**：启动 cache 的 informer（Pod、Node、PodGroup、Queue 等）和 StatusUpdater worker。Cache 是集群状态的权威数据源，由 Kubernetes API 的 Informer 构建（见 `scheduler-concepts-zh.md`「Cache」）。
- **`WaitForCacheSync(stopCh)`**：阻塞直到所有 informer 完成首次同步，确保 `runOnce` 执行时本地视图已与集群一致。
- **`wait.Until(s.runOnce, s.schedulePeriod, stopCh)`**：每隔 `schedulePeriod`（默认 1 秒）调用一次 `runOnce`，即一个**调度周期**。`stopCh` 关闭时退出。

---

#### 3.2.3 `runOnce`：单周期完整流程

```go
func (s *Scheduler) runOnce() {
	// 1. 为本次周期生成唯一 ID，用于日志关联和 trace
	sessionId := generateSessionID(6)
	log.InfraLogger.SetSessionID(string(sessionId))

	log.InfraLogger.V(1).Infof("Start scheduling ...")
	scheduleStartTime := time.Now()
	defer log.InfraLogger.V(1).Infof("End scheduling ...")
	// 2. 记录本周期从开始到结束的耗时，用于 Prometheus 指标
	defer metrics.UpdateE2eDuration(scheduleStartTime)

	// 3. OpenSession：从 cache 生成 Snapshot，创建 Session，加载插件（OnSessionOpen）
	//    Session 表示单个调度周期的调度上下文，包含 PodGroupInfos、Nodes、Queues 等一致视图，
	//    以及 JobOrder、TaskOrder、Predicate、NodeOrder 等插件扩展点
	ssn, err := framework.OpenSession(s.cache, s.config, s.schedulerParams, sessionId, s.mux)
	if err != nil {
		log.InfraLogger.Errorf("Error while opening session, will try again next cycle. \nCause: %+v", err)
		return
	}
	// 4. 周期结束时关闭 Session：触发 OnSessionClose，记录 Job 状态事件
	defer framework.CloseSession(ssn)

	// 5. 从 config 解析 actions 列表（如 "allocate, consolidate, reclaim, preempt, stalegangeviction"）
	actions, _ := conf_util.GetActionsFromConfig(s.config)
	for _, action := range actions {
		log.InfraLogger.SetAction(string(action.Name()))
		metrics.SetCurrentAction(string(action.Name()))
		actionStartTime := time.Now()
		// 6. 按顺序执行每个 Action，每个 Action 基于 Session 的 Snapshot 工作，使用 Statement 保证原子性
		action.Execute(ssn)
		metrics.UpdateActionDuration(string(action.Name()), metrics.Duration(actionStartTime))
	}
	log.InfraLogger.RemoveActionLogger()
}
```

---

#### 3.2.4 Actions 执行顺序与设计意图

`conf_util.GetActionsFromConfig` 解析 config 中的 `actions` 字符串（如 `"allocate, consolidate, reclaim, preempt, stalegangeviction"`），返回对应的 Action 实例列表。

执行顺序遵循 `docs/developer/action-framework-zh.md` 的设计，旨在**最小化干扰**：

| 顺序 | Action | 作用 |
|------|--------|------|
| 1 | **Allocate** | 常规调度，将待调度 Pod 分配到可用节点，无干扰 |
| 2 | **Consolidate** | 整合工作负载，减少资源碎片，优化放置 |
| 3 | **Reclaim** | 在队列之间回收借用的资源，实现公平分配 |
| 4 | **Preempt** | 队列内基于优先级的抢占，驱逐低优先级作业 |
| 5 | **StaleGangEviction** | 驱逐违反 minMember 的 gang 作业，防止集群死锁 |

此顺序确保仅在必要时执行破坏性操作，优先尝试干扰最小的选项。

---

#### 3.2.5 Statement 与事务模型

每个 Action 在处理 Job 时，会通过 `ssn.Statement()` 创建 Statement 对象。Statement 提供类似事务的机制（见 `scheduler-concepts-zh.md`「Statements 与事务模型」、`action-framework-zh.md`「Statement」）：

- **Checkpoint/Rollback**：创建回滚点，失败时可恢复
- **Allocate/Evict/Pipeline**：在内存中模拟调度决策，不直接写集群
- **Commit/Discard**：成功则提交到集群，失败则丢弃

这样可以在提交前评估潜在变更，保证调度决策的原子性。

---

## 4. `actions/allocate/allocate.go`：分配动作的入口

文件：`pkg/scheduler/actions/allocate/allocate.go`

### 4.1 Action 定义与设计思想

对应文档：`docs/developer/action-framework-zh.md`、`docs/developer/designs/node-sets/README-zh.md`、`docs/developer/designs/hierarchical-podgroup/README-zh.md`。

#### 4.1.1 Allocate 在 Action 框架中的定位

根据 `action-framework-zh.md`，**Allocate（分配）** 是五个调度动作中的**主动作**，也是**第一个执行**的动作：

- **主要职责**：尝试将待调度的 Pod 分配到可用节点
- **实现方式**：使用评分函数（NodeOrderFn 等）寻找 Pod 的最优放置位置
- **设计意图**：首先执行，以处理**无需干扰**即可调度的 Pod

动作链的设计原则是「最小化干扰」：Allocate 不驱逐任何已有 Pod，只做常规调度；若 Allocate 无法满足需求，后续的 Consolidate、Reclaim、Preempt 才会逐步引入更「破坏性」的操作。

#### 4.1.2 与相关设计文档的对应关系

| 设计文档 | 与 Allocate 的关系 |
|----------|-------------------|
| **action-framework-zh.md** | 定义 Allocate 的职责、执行顺序、以及 Scenarios/Simulation/Statement 的使用方式 |
| **node-sets/README-zh.md** | 定义 `SubsetNodesFn` 钩子：在尝试分配任务前，将集群节点划分为 NodeSet，逐个 NodeSet 尝试调度，直到成功。`common.AllocateJob` 中的 `allocateSubGroupSet` / `allocatePodSet` 会调用 `ssn.SubsetNodesFn` 获取候选节点集 |
| **hierarchical-podgroup/README-zh.md** | 定义 PodGroup 内的 SubGroup 层次结构：Allocate 按 SubGroupSet → PodSet → Task 递归分配，支持细粒度 gang 调度和拓扑约束 |

#### 4.1.3 代码：Action 接口实现

```go
type allocateAction struct{}

func New() *allocateAction {
	return &allocateAction{}
}

func (alloc *allocateAction) Name() framework.ActionType {
	return framework.Allocate
}
```

- `allocateAction` 实现 `framework.Action` 接口（`Name` + `Execute`），无状态，故结构体为空。
- `Name()` 返回 `framework.Allocate`，供 `conf_util.GetActionsFromConfig` 解析 config 中的 `"allocate"` 字符串并实例化。
- `New()` 由 `actions.InitDefaultActions()` 注册到全局，调度器启动时完成注册。

### 4.2 `Execute`：按队列顺序为 PodGroup 分配资源

对应文档：`docs/developer/plugin-framework-zh.md`（调度顺序）、`docs/queues/README.md`（队列层级）、`docs/fairness/README.md`（公平分配）、`docs/developer/action-framework-zh.md`（Statement、Pipeline）。

#### 4.2.1 整体流程

```go
func (alloc *allocateAction) Execute(ssn *framework.Session) {
	log.InfraLogger.V(2).Infof("Enter Allocate ...")
	defer log.InfraLogger.V(2).Infof("Leaving Allocate ...")

	// 1. 创建按队列顺序迭代 Job 的迭代器
	//    - FilterNonPending: 只调度有 Pending Pod 的 Job
	//    - FilterUnready: 只调度已就绪的 Job（如 gang 满足 minMember 等条件）
	//    - MaxJobsQueueDepth: 每队列每周期最多尝试的 Job 数，来自 config.QueueDepthPerAction["allocate"]
	jobsOrderByQueues := utils.NewJobsOrderByQueues(ssn, utils.JobsOrderInitOptions{
		FilterNonPending:  true,
		FilterUnready:     true,
		MaxJobsQueueDepth: ssn.GetJobsDepth(framework.Allocate),
	})
	// 2. 用 Session 中的 PodGroupInfos 初始化，过滤后按 Department → Queue → Job 顺序组织
	jobsOrderByQueues.InitializeWithJobs(ssn.PodGroupInfos)

	log.InfraLogger.V(2).Infof("There are <%d> PodGroupInfos and <%d> Queues in total for scheduling",
		jobsOrderByQueues.Len(), ssn.CountLeafQueues())

	// 3. 主循环：按队列公平顺序逐个尝试调度 Job
	for !jobsOrderByQueues.IsEmpty() {
		// 3.1 弹出下一个待调度 Job（按 QueueOrderFn、JobOrderFn 决定的顺序）
		job := jobsOrderByQueues.PopNextJob()
		// 3.2 为本 Job 创建 Statement，用于暂存 Allocate/Pipeline 等操作，支持 Commit/Discard
		stmt := ssn.Statement()
		// 3.3 记录本 Job 是否已有已分配 Pod（用于后续是否设置 LastStartTimestamp）
		alreadyAllocated := job.GetNumAllocatedTasks() > 0

		// 3.4 尝试分配：在内存中模拟调度，成功则 ok=true，若转为 pipeline 则 pipelined=true
		if ok, pipelined := attemptToAllocateJob(ssn, stmt, job); ok {
			metrics.IncPodgroupScheduledByAction()
			// 3.5 提交到集群：创建 BindRequest 等，将调度决策落盘
			err := stmt.Commit()
			// 3.6 若提交成功且为首次真正分配（非 pipeline、非已有分配），记录 Job 开始时间
			if err == nil && !pipelined && !alreadyAllocated {
				setLastStartTimestamp(job)
			}
			// 3.7 若提交成功且仍有待分配 Pod（如 gang 未满足），重新入队，下一轮继续
			if err == nil && podgroup_info.HasTasksToAllocate(job, true) {
				jobsOrderByQueues.PushJob(job)
				continue
			}
		} else {
			// 3.8 分配失败，丢弃本 Job 的 Statement，不影响集群状态
			stmt.Discard()
		}
	}
}
```

---

#### 4.2.2 `JobsOrderByQueues`：队列与 Job 的公平迭代

`JobsOrderByQueues` 是“按队列层级顺序”迭代 Job 的抽象，实现公平分配（见 `docs/fairness/README.md`、`docs/queues/README.md`）：

- **层级结构**：队列有 parent（父队列），形成层级。`PopNextJob` 按 Department → Queue → Job 的顺序弹出，保证同一队列内、同一层级内按优先级/公平份额调度。
- **插件扩展**：`QueueOrderFn`、`JobOrderFn` 由插件在 `OnSessionOpen` 中注册（见 `plugin-framework-zh.md`「调度顺序」），`JobsOrderByQueues` 内部会使用这些回调决定队列和 Job 的优先级。

`JobsOrderInitOptions` 含义：

| 选项 | 含义 |
|------|------|
| `FilterNonPending: true` | 仅调度有 Pending Pod 的 Job（`len(job.PodStatusIndex[Pending]) > 0`） |
| `FilterUnready: true` | 仅调度已就绪的 Job（`IsReadyForScheduling()` 为真，如 gang 未满足 minMember 则视为未就绪） |
| `MaxJobsQueueDepth` | 来自 `config.QueueDepthPerAction["allocate"]`，限制每队列每周期最多尝试的 Job 数，用于控制公平性与延迟 |

---

#### 4.2.3 主循环：Statement 事务与部分分配

| 步骤 | 代码 | 说明 |
|------|------|------|
| 1 | `job := jobsOrderByQueues.PopNextJob()` | 按队列顺序取下一个待调度 Job |
| 2 | `stmt := ssn.Statement()` | 创建 Statement，用于暂存本 Job 的 Allocate/Evict/Pipeline 等操作（见 `action-framework-zh.md`「Statement」） |
| 3 | `attemptToAllocateJob(...)` | 尝试分配，内部调用 `common.AllocateJob`，在内存中模拟调度 |
| 4 | `stmt.Commit()` / `stmt.Discard()` | 成功则提交到集群（创建 BindRequest 等），失败则丢弃 |
| 5 | `setLastStartTimestamp(job)` | 首次成功分配且非 pipeline 时，记录 Job 开始时间 |
| 6 | `HasTasksToAllocate(job, true)` 为真时 `PushJob(job)` | 若本次只分配了部分 Pod（如 gang 未满足），将 Job 重新入队，下一轮继续 |

**部分分配**：一次调度周期可能只为 Job 分配部分 Pod（例如 gang 需 8 个 Pod 但只分配了 3 个）。此时将 Job 重新 `PushJob`，下次 `PopNextJob` 会再次尝试，直到全部分配或无法继续。

---

#### 4.2.4 `attemptToAllocateJob`：调用公共分配逻辑

```go
func attemptToAllocateJob(ssn *framework.Session, stmt *framework.Statement, job *podgroup_info.PodGroupInfo) (allocated, pipelined bool) {
	queue := ssn.Queues[job.Queue]

	// 1. 使用 PodSetOrderFn、TaskOrderFn 确定待分配 Pod 的集合与顺序，用于日志和资源统计
	resReq := podgroup_info.GetTasksToAllocateInitResource(job, ssn.PodSetOrderFn, ssn.TaskOrderFn, true)
	log.InfraLogger.V(3).Infof("Attempting to allocate job: <%v/%v> of queue <%v>, resources: <%v>",
		job.Namespace, job.Name, queue.Name, resReq)

	// 2. 调用公共分配逻辑：isPipelineOnly=false 表示允许真正 Allocate（绑定）或 Pipeline
	//    内部会 allocateSubGroupSet → SubsetNodesFn 获取 NodeSet → 按 SubGroup 层次递归分配
	nodes := maps.Values(ssn.Nodes)
	if !common.AllocateJob(ssn, stmt, nodes, job, false) {
		log.InfraLogger.V(3).Infof("Could not allocate resources for job: <%v/%v> of queue <%v>",
			job.Namespace, job.Name, job.Queue)
		return false, false
	}

	// 3. 若 Job 需要 pipeline（如 gang 未满足、部分 Pod 在“待驱逐”资源上分配），
	//    则把本次已 Allocate 的 Pod 全部转为 Pipelined，避免提前绑定导致 gang 语义不一致
	//    Pipeline 表示“在待驱逐资源上分配”，Pod 被标记为将来可调度到该节点，但资源尚未真正释放
	pipelined = false
	if job.ShouldPipelineJob() {
		log.InfraLogger.V(3).Infof(
			"Some tasks were pipelined, setting all job to be pipelined for job: <%v/%v>",
			job.Namespace, job.Name)
		err := stmt.ConvertAllAllocatedToPipelined(job.UID)
		if err != nil {
			log.InfraLogger.Errorf(
				"Failed to covert tasks from allocated to pipelined for job: <%v/%v>, error: <%v>",
				job.Namespace, job.Name, err)
			return false, false
		}
		pipelined = true
	} else {
		// 4. 真正分配成功：Commit 后 Binder 会创建 BindRequest，Pod 将绑定到节点
		log.InfraLogger.V(3).Infof("Succesfully allocated resources for job: <%v/%v>",
			job.Namespace, job.Name)
	}

	return true, pipelined
}
```

---

## 5. `actions/common/allocate.go`：具体的分配算法

文件：`pkg/scheduler/actions/common/allocate.go`

对应设计文档：
- **hierarchical-podgroup**（`docs/developer/designs/hierarchical-podgroup/README-zh.md`）：SubGroup、SubGroupSet、PodSet 层次结构，minMember、拓扑约束
- **node-sets**（`docs/developer/designs/node-sets/README-zh.md`）：SubsetNodesFn 钩子、NodeSet、多域尝试
- **action-framework**（`docs/developer/action-framework-zh.md`）：Statement、Checkpoint/Rollback、Allocate/Pipeline
- **plugin-framework**（`docs/developer/plugin-framework-zh.md`）：PrePredicateFn、PredicateFn、NodeOrderFn

---

### 5.1 `AllocateJob`：Job 级别入口

```go
func AllocateJob(ssn *framework.Session, stmt *framework.Statement, nodes []*node_info.NodeInfo,
	job *podgroup_info.PodGroupInfo, isPipelineOnly bool) bool {
	// 1. Job 级别预处理（插件可在此做预计算、状态更新等）
	ssn.PreJobAllocation(job)

	// 2. 获取待分配 Pod 列表，使用 PodSetOrderFn、TaskOrderFn 确定顺序（plugin-framework 扩展点）
	tasksToAllocate := podgroup_info.GetTasksToAllocate(job, ssn.PodSetOrderFn, ssn.TaskOrderFn, !isPipelineOnly)

	// 3. 队列容量检查：判断 Job 是否超过队列配额/公平份额（fairness、queues 设计）
	result := ssn.IsJobOverQueueCapacityFn(job, tasksToAllocate)
	if !result.IsSchedulable {
		if !isPipelineOnly {
			job.SetJobFitError(result.Reason, result.Message, result.Details)
		}
		return false
	}
	// 4. 进入层级分配：从 RootSubGroupSet 开始递归（hierarchical-podgroup 设计）
	return allocateSubGroupSet(ssn, stmt, nodes, job, job.RootSubGroupSet, tasksToAllocate, isPipelineOnly)
}
```

### 5.2 层级分配：SubGroupSet、PodSet 与 NodeSet

> **设计文档对应**：`hierarchical-podgroup`（SubGroup 层次、minMember、拓扑约束）、`node-sets`（SubsetNodesFn、NodeSet、多域尝试）、`action-framework`（Checkpoint/Rollback）

#### 5.2.1 `allocateSubGroupSet`：NodeSet 划分与多域尝试

```go
func allocateSubGroupSet(ssn *framework.Session, stmt *framework.Statement, nodes []*node_info.NodeInfo,
	job *podgroup_info.PodGroupInfo, subGroupSet *subgroup_info.SubGroupSet, tasksToAllocate []*pod_info.PodInfo,
	isPipelineOnly bool,
) bool {
	// 1. SubsetNodesFn 钩子（node-sets 设计）：将节点划分为 NodeSet 列表，支持拓扑等多域尝试
	//    多插件时分层划分，如 P: AllNodes->P1,P2,P3，R: P1->P1R1,P1R2，最终尝试 P1R1,P1R2,P2R1...
	nodeSets, err := ssn.SubsetNodesFn(job, &subGroupSet.SubGroupInfo, subGroupSet.GetAllPodSets(), tasksToAllocate, nodes)
	if err != nil {
		return false
	}

	// 2. 逐个 NodeSet 尝试，直到成功（node-sets：传统单域失败即放弃，现改为多域尝试）
	for _, nodeSet := range nodeSets {
		cp := stmt.Checkpoint()  // action-framework：创建回滚点
		if allocateSubGroupSetOnNodes(ssn, stmt, nodeSet, job, subGroupSet, tasksToAllocate, isPipelineOnly) {
			return true
		}
		if err := stmt.Rollback(cp); err != nil { ... }  // 失败则回滚，尝试下一 NodeSet
	}
	return false
}
```

#### 5.2.2 `allocateSubGroupSetOnNodes`：SubGroup 层次递归

```go
func allocateSubGroupSetOnNodes(ssn *framework.Session, stmt *framework.Statement, nodes node_info.NodeSet,
	job *podgroup_info.PodGroupInfo, subGroupSet *subgroup_info.SubGroupSet, tasksToAllocate []*pod_info.PodInfo,
	isPipelineOnly bool,
) bool {
	// 1. 先递归分配子 SubGroupSet（hierarchical-podgroup：SubGroup 有 parent，形成树形结构）
	for _, childSubGroupSet := range orderedSubGroupSets(ssn, subGroupSet.GetChildGroups()) {
		podSets := childSubGroupSet.GetAllPodSets()
		subGroupTasks := filterTasksForPodSets(podSets, tasksToAllocate)
		if !allocateSubGroupSet(ssn, stmt, nodes, job, childSubGroupSet, subGroupTasks, isPipelineOnly) {
			return false
		}
	}

	// 2. 再分配当前层的 PodSet（PodSet 是叶节点，对应同一资源模板的 Pod 集合）
	for _, podSet := range orderedPodSets(ssn, subGroupSet.GetChildPodSets()) {
		podSetTasks := filterTasksForPodSet(podSet, tasksToAllocate)
		if !allocatePodSet(ssn, stmt, nodes, job, podSet, podSetTasks, isPipelineOnly) {
			return false
		}
	}
	return true
}
```

#### 5.2.3 `allocatePodSet`：PodSet 级 NodeSet 与逐 Task 分配

`allocatePodSet` 对 PodSet 再次调用 `SubsetNodesFn`（拓扑等插件可能对 PodSet 有更细粒度划分），然后对每个 NodeSet 调用 `allocateTasksOnNodeSet` 逐 Pod 分配。结构同 `allocateSubGroupSet`：`Checkpoint` → 尝试 → 失败则 `Rollback`。

### 5.3 单 Pod 分配：`allocateTask` 与插件扩展点

> **设计文档对应**：`plugin-framework`（PrePredicateFn、PredicateFn、NodeOrderFn）、`hierarchical-podgroup`（gang 失败时 fit error）

```go
func allocateTasksOnNodeSet(ssn *framework.Session, stmt *framework.Statement, nodes node_info.NodeSet,
	job *podgroup_info.PodGroupInfo, tasksToAllocate []*pod_info.PodInfo, isPipelineOnly bool) bool {
	for index, task := range tasksToAllocate {
		success := allocateTask(ssn, stmt, nodes, task, isPipelineOnly)
		if !success {
			// gang 调度失败时设置 Job 级 fit error（hierarchical-podgroup：minMember 未满足）
			handleFailedTaskAllocation(job, task, index)
			return false
		}
	}
	return true
}

func allocateTask(ssn *framework.Session, stmt *framework.Statement, nodes []*node_info.NodeInfo,
	task *pod_info.PodInfo, isPipelineOnly bool) (success bool) {
	job := ssn.PodGroupInfos[task.Job]
	// 1. PrePredicateFn（plugin-framework）：主谓词前的快速检查
	err := ssn.PrePredicateFn(task, job)
	if err != nil {
		job.SetTaskFitError(task, fitErrors)
		return false
	}

	// 2. OrderedNodesByTask：聚合各插件的 NodeOrderFn 对节点排序（装箱、亲和性、拓扑等）
	orderedNodes := ssn.OrderedNodesByTask(nodes, task)
	for _, node := range orderedNodes {
		// 3. FittingNode：执行 PredicateFn，检查资源、约束是否满足
		if !ssn.FittingNode(task, node, !isPipelineOnly) {
			continue
		}
		success = allocateTaskToNode(ssn, stmt, task, node, isPipelineOnly)
		if success {
			break
		}
	}
	return success
}
```

### 5.4 Allocate vs Pipeline：`allocateTaskToNode`

> **设计文档对应**：`action-framework`（Allocate vs Pipeline）、`docs/gpu-sharing/README.md`（分式 GPU）

```go
func allocateTaskToNode(ssn *framework.Session, stmt *framework.Statement, task *pod_info.PodInfo, node *node_info.NodeInfo, isPipelineOnly bool) bool {
	// 1. 分式 GPU / GPU 内存请求：交给 gpu_sharing 模块（docs/gpu-sharing）
	if task.IsFractionRequest() || task.IsMemoryRequest() {
		return gpu_sharing.AllocateFractionalGPUTaskToNode(ssn, stmt, task, node, isPipelineOnly)
	}

	// 2. 节点当前有足够资源：stmt.Allocate 真正绑定（action-framework：Commit 后落盘）
	if taskAllocatable := node.IsTaskAllocatable(task); !isPipelineOnly && taskAllocatable {
		return bindTaskToNode(ssn, stmt, task, node)
	}
	// 3. 否则：stmt.Pipeline，在待驱逐资源上预占位，资源释放后再绑定（action-framework）
	return pipelineTaskToNode(ssn, stmt, task, node, !isPipelineOnly)
}
```

---

## 6. 小结：主线回顾

- **进程入口**：`main.go` → `app.RunApp`。
- **应用初始化**：`RunApp` 解析参数、创建 K8s client / HTTP server → `Run`。
- **Scheduler 构造与 leader election**：`Run` 注册 actions / plugins，解析 scheduler 配置，构造 `Scheduler`，再通过 Kubernetes leader election 决定哪个实例执行调度循环。
- **调度循环**：`Scheduler.Run` 启动 cache，同步完成后按周期调用 `runOnce`。
- **Session & Action 链**：`runOnce` 打开 `Session`，从配置中取出 actions 列表（包含 `Allocate`），按顺序调用 `Execute`。
- **Allocate 动作**：`actions/allocate` 负责按队列公平地取出 PodGroup，使用 `Statement` 事务式尝试分配，底层调用 `actions/common/allocate` 完成层级 PodGroup / PodSet / 单 Pod 的资源选择与绑定或 pipeline。
